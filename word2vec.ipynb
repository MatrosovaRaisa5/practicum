{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2e22de-5a33-4f50-b450-2e4ebc2d093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: razdel in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: pymorphy2 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: click in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: colorama in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from tensorboard) (5.29.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from tensorboard) (79.0.1)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from tensorboard) (3.0.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\zver\\desktop\\machine_learning\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch razdel pymorphy2 nltk tqdm tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5846ca4-a5c3-4ff7-b95d-039565eabf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from razdel import tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc0fff2-f221-4230-a1d6-1b46554ca1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Zver\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.morph = MorphAnalyzer()\n",
    "        self.russian_stopwords = set(stopwords.words('russian'))\n",
    "        self.custom_stopwords = {\n",
    "            'это', 'то', 'как', 'так', 'и', 'в', 'над', 'к', 'до', 'не', 'на', 'но', 'за', 'то', \n",
    "            'же', 'вы', 'бы', 'по', 'только', 'его', 'мне', 'было', 'вот', 'от', 'меня', 'еще', \n",
    "            'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'ли', 'если', 'уже', 'или', \n",
    "            'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', \n",
    "            'там', 'потом', 'себя', 'ничего', 'который', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', \n",
    "            'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', \n",
    "            'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', \n",
    "            'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', \n",
    "            'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', \n",
    "            'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', \n",
    "            'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', \n",
    "            'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', \n",
    "            'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'вдруг', 'сегодня', 'тот'\n",
    "        }\n",
    "        self.all_stopwords = self.russian_stopwords.union(self.custom_stopwords)\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Приведение к нижнему регистру\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Удаление email адресов\n",
    "        text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "        \n",
    "        # Удаление URL\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        \n",
    "        # Удаление цифр\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Удаление пунктуации и специальных символов\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Удаление лишних пробелов\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def lemmatize_tokens(self, tokens):\n",
    "        \"\"\"Лемматизация токенов\"\"\"\n",
    "        lemmas = []\n",
    "        for token in tokens:\n",
    "            # Пропускаем стоп-слова и короткие токены\n",
    "            if (len(token) <= 2 or \n",
    "                token in self.all_stopwords or \n",
    "                not re.match(r'^[а-яё]+$', token)):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                parsed = self.morph.parse(token)[0]\n",
    "                lemma = parsed.normal_form\n",
    "                # Проверяем, что лемма состоит только из русских букв\n",
    "                if re.match(r'^[а-яё]+$', lemma) and len(lemma) > 1:\n",
    "                    lemmas.append(lemma)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return lemmas\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        tokens = [token.text for token in tokenize(cleaned_text)]\n",
    "        \n",
    "        lemmas = self.lemmatize_tokens(tokens)\n",
    "        \n",
    "        return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9923a8e-5f31-46cd-8550-670bac044108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Класс для работы со словарем\"\"\"\n",
    "    \n",
    "    def __init__(self, min_freq=5):\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_freq = {}\n",
    "        self.unknown_token = '<UNK>'\n",
    "        self.padding_token = '<PAD>'\n",
    "        \n",
    "    def build_vocabulary(self, texts):\n",
    "        \"\"\"Построение словаря на основе текстов\"\"\"\n",
    "        print(\"Построение словаря\")\n",
    "        \n",
    "        # Подсчет частот слов\n",
    "        word_counts = Counter()\n",
    "        for text in tqdm(texts, desc=\"Подсчет частот слов\"):\n",
    "            word_counts.update(text)\n",
    "        \n",
    "        # Фильтрация по минимальной частоте\n",
    "        filtered_words = {word: count for word, count in word_counts.items() \n",
    "                         if count >= self.min_freq}\n",
    "        \n",
    "        # Создание словаря\n",
    "        self.word2idx = {self.padding_token: 0, self.unknown_token: 1}\n",
    "        self.idx2word = {0: self.padding_token, 1: self.unknown_token}\n",
    "        \n",
    "        for idx, word in enumerate(filtered_words.keys(), start=2):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "        \n",
    "        self.word_freq = filtered_words\n",
    "        \n",
    "        print(f\"Размер словаря: {len(self.word2idx)} слов\")\n",
    "        print(f\"Наиболее частые слова: {list(filtered_words.items())[:10]}\")\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        \"\"\"Кодирование текста в индексы\"\"\"\n",
    "        return [self.word2idx.get(word, self.word2idx[self.unknown_token]) \n",
    "                for word in text if word in self.word2idx]\n",
    "    \n",
    "    def get_word_list(self):\n",
    "        \"\"\"Получение списка слов в порядке индексов\"\"\"\n",
    "        words = []\n",
    "        for i in range(len(self.idx2word)):\n",
    "            words.append(self.idx2word[i])\n",
    "        return words\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15a1a845-1eed-41c7-ab16-2c15f3e34c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs) \n",
    "        embeds = torch.mean(embeds, dim=1)\n",
    "        out = self.linear(embeds) # Предсказываем целевое слово\n",
    "        return out\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, target):\n",
    "        embeds = self.embeddings(target)\n",
    "        out = self.linear(embeds) # Предсказываем контекстые слова\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccc2bd36-cfd1-4a68-b9a2-0dc2072123aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    \"\"\"Датасет для Word2Vec моделей\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, vocab, window_size=5, model_type='cbow'):\n",
    "        self.data = []\n",
    "        self.model_type = model_type\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        print(f\"Создание датасета для {model_type.upper()}...\")\n",
    "        \n",
    "        for text in tqdm(texts, desc=\"Обработка текстов\"):\n",
    "            encoded_text = vocab.encode_text(text)\n",
    "            \n",
    "            if len(encoded_text) < window_size * 2 + 1:\n",
    "                continue\n",
    "            \n",
    "            if model_type == 'cbow':\n",
    "                self._create_cbow_samples(encoded_text)\n",
    "            else:\n",
    "                self._create_skipgram_samples(encoded_text)\n",
    "    \n",
    "    def _create_cbow_samples(self, encoded_text):\n",
    "        \"\"\"Создание samples для CBOW\"\"\"\n",
    "        for i in range(self.window_size, len(encoded_text) - self.window_size):\n",
    "            context = (encoded_text[i-self.window_size:i] + \n",
    "                      encoded_text[i+1:i+self.window_size+1])\n",
    "            target = encoded_text[i]\n",
    "            self.data.append((torch.tensor(context), torch.tensor(target)))\n",
    "    \n",
    "    def _create_skipgram_samples(self, encoded_text):\n",
    "        \"\"\"Создание samples для SkipGram\"\"\"\n",
    "        for i in range(self.window_size, len(encoded_text) - self.window_size):\n",
    "            target = encoded_text[i]    # Целевое слово\n",
    "            # Для каждого слова в окне создаем пару (target, context_word)\n",
    "            for j in range(i-self.window_size, i+self.window_size+1):\n",
    "                if j != i and 0 <= j < len(encoded_text):\n",
    "                    self.data.append((torch.tensor(target), torch.tensor(encoded_text[j])))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "023a3874-c9f7-46a6-b998-c87f078b3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecTrainer:\n",
    "    \"\"\"Класс для обучения моделей Word2Vec\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=100, learning_rate=0.001, weight_decay=1e-5, log_dir=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.writer = SummaryWriter(log_dir) if log_dir else None\n",
    "        \n",
    "    def train_cbow(self, dataloader, epochs=10, model_name=\"CBOW\"):\n",
    "        \"\"\"Обучение CBOW модели\"\"\"\n",
    "        print(f\"Обучение {model_name} модели\")\n",
    "        model = CBOWModel(self.vocab_size, self.embedding_dim)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            for batch_idx, (context, target) in enumerate(progress_bar):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(context)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "                \n",
    "                if self.writer and batch_idx % 100 == 0:\n",
    "                    self.writer.add_scalar(f'{model_name}/batch_loss', loss.item(), \n",
    "                                         epoch * len(dataloader) + batch_idx)\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "            if self.writer:\n",
    "                self.writer.add_scalar(f'{model_name}/epoch_loss', avg_loss, epoch)\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def train_skipgram(self, dataloader, epochs=10, model_name=\"SkipGram\"):\n",
    "        \"\"\"Обучение SkipGram модели\"\"\"\n",
    "        print(f\"Обучение {model_name} модели\")\n",
    "        model = SkipGramModel(self.vocab_size, self.embedding_dim)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            for batch_idx, (target, context) in enumerate(progress_bar):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(target)\n",
    "                loss = criterion(output, context)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "                \n",
    "                if self.writer and batch_idx % 100 == 0:\n",
    "                    self.writer.add_scalar(f'{model_name}/batch_loss', loss.item(), \n",
    "                                         epoch * len(dataloader) + batch_idx)\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "            \n",
    "            if self.writer:\n",
    "                self.writer.add_scalar(f'{model_name}/epoch_loss', avg_loss, epoch)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f78c22f0-ccee-4974-9a76-26d96666562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЗАГРУЗКА И ПРЕДОБРАБОТКА ДАННЫХ\n",
      "Загрузка данных\n",
      "Загружено 5000 текстов\n",
      "Предобработка текстов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка текстов: 100%|██████████████████████████████████████████████████████████| 5000/5000 [00:05<00:00, 958.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "После фильтрации осталось 2328 текстов\n",
      "Построение словаря\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Подсчет частот слов: 100%|█████████████████████████████████████████████████████| 2328/2328 [00:00<00:00, 382240.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 2043 слов\n",
      "Наиболее частые слова: [('просить', 385), ('убрать', 204), ('дерево', 49), ('кустарник', 11), ('который', 109), ('выйти', 8), ('предел', 10), ('газон', 189), ('пешеходный', 62), ('зона', 25)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = r'C:\\Users\\Zver\\Desktop\\machine_learning\\data\\Petitions.csv'\n",
    "SAMPLE_SIZE = 5000 \n",
    "EMBEDDING_DIM = 200\n",
    "WINDOW_SIZE = 5\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 15\n",
    "MIN_WORD_FREQ = 3\n",
    "\n",
    "# Создание директории для логов\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = f\"runs/word2vec_{timestamp}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "print(\"ЗАГРУЗКА И ПРЕДОБРАБОТКА ДАННЫХ\")\n",
    "\n",
    "print(\"Загрузка данных\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "texts = df['public_petition_text'].dropna().astype(str).tolist()[:SAMPLE_SIZE]\n",
    "print(f\"Загружено {len(texts)} текстов\")\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "processed_texts = []\n",
    "\n",
    "print(\"Предобработка текстов\")\n",
    "for text in tqdm(texts, desc=\"Обработка текстов\"):\n",
    "    processed = preprocessor.preprocess(text)\n",
    "    if len(processed) > 5:  # Пропускаем слишком короткие тексты\n",
    "        processed_texts.append(processed)\n",
    "\n",
    "print(f\"После фильтрации осталось {len(processed_texts)} текстов\")\n",
    "\n",
    "vocab = Vocabulary(min_freq=MIN_WORD_FREQ)\n",
    "vocab.build_vocabulary(processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb3a279b-df0e-45ea-a076-e1ced07507a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создание датасета для CBOW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка текстов: 100%|████████████████████████████████████████████████████████| 2328/2328 [00:00<00:00, 13670.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создание датасета для SKIPGRAM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка текстов: 100%|█████████████████████████████████████████████████████████| 2328/2328 [00:01<00:00, 1248.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW samples: 15017\n",
      "SkipGram samples: 150170\n",
      "Обучение CBOW модели\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████████████████████████████████████████████████| 118/118 [00:00<00:00, 151.49it/s, loss=7.1241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 7.4163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████████████████████████████████████████████████| 118/118 [00:00<00:00, 193.31it/s, loss=6.4177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 6.7247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████████████████████████████████████████████████| 118/118 [00:00<00:00, 179.46it/s, loss=6.2486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 6.1337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████████████████████████████████████████████████| 118/118 [00:00<00:00, 166.47it/s, loss=5.9421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Loss: 5.7187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████████████████████████████████████████████████| 118/118 [00:00<00:00, 177.62it/s, loss=5.2912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Loss: 5.3713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████████████████████████████████████████████████| 118/118 [00:00<00:00, 222.22it/s, loss=5.3278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Loss: 5.0498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████████████████████████████████████████████████| 118/118 [00:00<00:00, 191.12it/s, loss=4.7486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Average Loss: 4.7401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████████████████████████████████████████████████| 118/118 [00:00<00:00, 179.09it/s, loss=4.2431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Average Loss: 4.4490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████████████████████████████████████████████████| 118/118 [00:00<00:00, 167.51it/s, loss=4.3141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Average Loss: 4.1814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|█████████████████████████████████████████████████████| 118/118 [00:00<00:00, 167.62it/s, loss=3.7408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Loss: 3.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|█████████████████████████████████████████████████████| 118/118 [00:00<00:00, 187.52it/s, loss=3.7024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Average Loss: 3.6996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|█████████████████████████████████████████████████████| 118/118 [00:00<00:00, 202.69it/s, loss=3.5953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Average Loss: 3.4872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|█████████████████████████████████████████████████████| 118/118 [00:00<00:00, 170.23it/s, loss=3.2340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Average Loss: 3.2916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|█████████████████████████████████████████████████████| 118/118 [00:00<00:00, 170.86it/s, loss=3.1271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Average Loss: 3.1114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|█████████████████████████████████████████████████████| 118/118 [00:00<00:00, 205.05it/s, loss=3.0836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Average Loss: 2.9464\n",
      "Обучение SkipGram модели\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|████████████████████████████████████████████████████| 1174/1174 [00:05<00:00, 208.33it/s, loss=6.1162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 7.1082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|████████████████████████████████████████████████████| 1174/1174 [00:05<00:00, 197.21it/s, loss=6.2065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 6.2263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|████████████████████████████████████████████████████| 1174/1174 [00:06<00:00, 194.85it/s, loss=6.0325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 5.8693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|████████████████████████████████████████████████████| 1174/1174 [00:05<00:00, 223.35it/s, loss=5.8145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Loss: 5.6521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|████████████████████████████████████████████████████| 1174/1174 [00:05<00:00, 207.26it/s, loss=5.9822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Loss: 5.5016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|████████████████████████████████████████████████████| 1174/1174 [00:05<00:00, 231.25it/s, loss=5.3762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Loss: 5.3904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|████████████████████████████████████████████████████| 1174/1174 [00:05<00:00, 197.28it/s, loss=5.2770]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Average Loss: 5.3054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|████████████████████████████████████████████████████| 1174/1174 [00:06<00:00, 184.82it/s, loss=5.0728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Average Loss: 5.2384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|████████████████████████████████████████████████████| 1174/1174 [00:06<00:00, 193.25it/s, loss=5.4540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Average Loss: 5.1835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|███████████████████████████████████████████████████| 1174/1174 [00:04<00:00, 247.92it/s, loss=5.6911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Loss: 5.1379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|███████████████████████████████████████████████████| 1174/1174 [00:05<00:00, 212.04it/s, loss=5.2165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Average Loss: 5.0979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|███████████████████████████████████████████████████| 1174/1174 [00:05<00:00, 218.21it/s, loss=5.6948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Average Loss: 5.0652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|███████████████████████████████████████████████████| 1174/1174 [00:04<00:00, 245.74it/s, loss=4.9210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Average Loss: 5.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|███████████████████████████████████████████████████| 1174/1174 [00:06<00:00, 189.31it/s, loss=5.0475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Average Loss: 5.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|███████████████████████████████████████████████████| 1174/1174 [00:05<00:00, 204.43it/s, loss=5.1953]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Average Loss: 4.9867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Создание датасетов и даталоадеров\n",
    "cbow_dataset = Word2VecDataset(processed_texts, vocab, WINDOW_SIZE, 'cbow')\n",
    "skipgram_dataset = Word2VecDataset(processed_texts, vocab, WINDOW_SIZE, 'skipgram')\n",
    "\n",
    "cbow_loader = DataLoader(cbow_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "skipgram_loader = DataLoader(skipgram_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"CBOW samples: {len(cbow_dataset)}\")\n",
    "print(f\"SkipGram samples: {len(skipgram_dataset)}\")\n",
    "\n",
    "# Обучение\n",
    "trainer = Word2VecTrainer(len(vocab), EMBEDDING_DIM, log_dir=log_dir)\n",
    "\n",
    "cbow_model = trainer.train_cbow(cbow_loader, EPOCHS, \"CBOW\")\n",
    "skipgram_model = trainer.train_skipgram(skipgram_loader, EPOCHS, \"SkipGram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03c33b9b-0482-41ed-a5c5-b011201def79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ВИЗУАЛИЗАЦИЯ В TENSORBOARD\n",
      "Размер матрицы эмбеддингов: torch.Size([2043, 200])\n",
      "Количество меток: 2043\n",
      "Добавление эмбеддингов в TensorBoard\n"
     ]
    }
   ],
   "source": [
    "print(\"ВИЗУАЛИЗАЦИЯ В TENSORBOARD\")\n",
    "\n",
    "# Создание writer для TensorBoard\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# Извлечение эмбеддингов\n",
    "cbow_embeddings = cbow_model.linear.weight.data.cpu()\n",
    "skipgram_embeddings = skipgram_model.linear.weight.data.cpu()\n",
    "\n",
    "# Подготовка метаданных\n",
    "metadata = []\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab.idx2word[i]\n",
    "    freq = vocab.word_freq.get(word, 0)\n",
    "    metadata.append(f\"{word} (freq: {freq})\")\n",
    "\n",
    "print(f\"Размер матрицы эмбеддингов: {cbow_embeddings.shape}\")\n",
    "print(f\"Количество меток: {len(metadata)}\")\n",
    "\n",
    "# Добавление эмбеддингов в TensorBoard \n",
    "print(\"Добавление эмбеддингов в TensorBoard\")\n",
    "\n",
    "# CBOW эмбеддинги\n",
    "writer.add_embedding(\n",
    "    cbow_embeddings,\n",
    "    metadata=metadata,\n",
    "    tag=\"CBOW Embeddings\",\n",
    "    global_step=0\n",
    ")\n",
    "\n",
    "# SkipGram эмбеддинги\n",
    "writer.add_embedding(\n",
    "    skipgram_embeddings,\n",
    "    metadata=metadata,\n",
    "    tag=\"SkipGram Embeddings\", \n",
    "    global_step=0\n",
    ")\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=50)\n",
    "cbow_pca = pca.fit_transform(cbow_embeddings.numpy())\n",
    "skipgram_pca = pca.fit_transform(skipgram_embeddings.numpy())\n",
    "\n",
    "writer.add_embedding(\n",
    "    torch.tensor(cbow_pca),\n",
    "    metadata=metadata,\n",
    "    tag=\"CBOW PCA-50\",\n",
    "    global_step=0\n",
    ")\n",
    "\n",
    "writer.add_embedding(\n",
    "    torch.tensor(skipgram_pca),\n",
    "    metadata=metadata,\n",
    "    tag=\"SkipGram PCA-50\",\n",
    "    global_step=0\n",
    ")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0876af73-5fc5-4099-9199-7b87c9ed2351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение моделей\n",
    "torch.save({\n",
    "    'cbow_model': cbow_model.state_dict(),\n",
    "    'skipgram_model': skipgram_model.state_dict(),\n",
    "    'vocab': vocab,\n",
    "    'embeddings_dim': EMBEDDING_DIM\n",
    "}, f'{log_dir}/word2vec_models.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d85177c-2d23-45b6-8089-4dd89c66a48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5f787d89ac490638\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5f787d89ac490638\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b61278-026e-4c7f-84a2-9cf9e9dcb298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0d1f0-1f1c-4b49-baf4-d0d8eb700cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79adb62-233d-4b38-9657-dc6e4e82121a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce049708-3175-4670-8e4c-313da6f00d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484903c5-7722-4716-9edf-8dc872b4c259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d5662-79a6-40b9-8d97-4cc0612dc94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
